| Public Dataset Links | Reference Number | Category | Author | Paper Title |
|-----------------------|------------------|----------|--------|-------------|
| http://www-rech.telecom-lille.fr/shrec2017-hand/ | 58 | Gesture | J. Shen, et al | Gesture spotter: A rapid prototyping tool for key gesture spotting in virtual and augmented reality applications. |
| https://univr-vips.github.io/Shrec21/ | 58 | Gesture | J. Shen, et al | Gesture spotter: A rapid prototyping tool for key gesture spotting in virtual and augmented reality applications. |
| http://www-rech.telecom-lille.fr/DHGdataset/ | 58 | Gesture | J. Shen, et al | Gesture spotter: A rapid prototyping tool for key gesture spotting in virtual and augmented reality applications. |
| https://www.repository.cam.ac.uk/items/487507e4-0a90-47df-bc8b-8b239105b416 | 58 | Gesture | J. Shen, et al | Gesture spotter: A rapid prototyping tool for key gesture spotting in virtual and augmented reality applications. |
| https://www.visuallocalization.net/datasets/ | 71 | Gesture | H. Yu, et al | Learning bipartite graph matching for robust visual localization |
| https://robotcar-dataset.robots.ox.ac.uk/ | 71 | Gesture | H. Yu, et al | Learning bipartite graph matching for robust visual localization |
| https://www.repository.cam.ac.uk/items/53788265-cb98-42ee-b85b-7a0cbc8eddb3 | 71 | Gesture | H. Yu, et al | Learning bipartite graph matching for robust visual localization |
| https://drive.google.com/file/d/12unZSgqQJyuCQVfC8SbO8n7eiDLRgbf8/view | 31 | Cybersickness | R. K. Kundu et al | Litevr: Interpretable and lightweight cybersickness detection using explainable ai. |
| https://github.com/tmp1986/UFFCSData | 30 | Cybersickness | R. K. Kundu et al | Truvr: Trustworthy cybersickness detection using explainable machine learning |
| https://github.com/shovonis/CyberSicknessClassification/tree/master/severity_classfication/data | 30 | Cybersickness | R. K. Kundu et al | Truvr: Trustworthy cybersickness detection using explainable machine learning |
| https://cvg.cit.tum.de/data/datasets/rgbd-dataset | 2 | Cybersickness | S. Balasubramanian et al | Prediction of discomfort due to egomotion in immersive videos for virtual reality. |
| http://sintel.is.tue.mpg.de/ | 2 | Cybersickness | S. Balasubramanian et al | Prediction of discomfort due to egomotion in immersive videos for virtual reality. |
| https://rpg.ifi.uzh.ch/davis_data.html | 2 | Cybersickness | S. Balasubramanian et al | Prediction of discomfort due to egomotion in immersive videos for virtual reality. |
| https://www.cvlibs.net/datasets/kitti/ | 2 | Cybersickness | S. Balasubramanian et al | Prediction of discomfort due to egomotion in immersive videos for virtual reality. |
| https://rpg.ifi.uzh.ch/zurichmavdataset.html | 2 | Cybersickness | S. Balasubramanian et al | Prediction of discomfort due to egomotion in immersive videos for virtual reality. |
| https://github.com/Terascale-All-sensing-Research-Studio/VR-Biometric-Authentication | 44 | Privacy/Security | R. Miller et all | Combining real-world constraints on user behavior with deep neural networks for virtual reality (vr) biometrics |
| https://github.com/Terascale-All-sensing-Research-Studio/VR-Biometric-Authentication | 45 | Privacy/Security | R. Miller et all | Temporal effects in motion behavior for virtual reality (vr) biometrics |
| https://chinapku-my.sharepoint.com/personal/1701111311_pku_edu_cn/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1701111311%5Fpku%5Fedu%5Fcn%2FDocuments%2FDGazeDataset&ga=1 | 21 | Tracking | Z. Hu et al | gaze: Cnn-based gaze prediction in dynamic scenes |
| https://ar5iv.labs.arxiv.org/html/2005.03876 | 16 | Tracking | Y. Feng et al | Real-time gaze tracking with event-driven eye segmentation |
| https://opendatascience.com/teyed-researchers-compile-20-million-images-of-eyes-in-a-massive-open-source-dataset/ | 16 | Tracking | Y. Feng et al | Real-time gaze tracking with event-driven eye segmentation |
| https://sites.google.com/nvidia.com/nvgaze | 29 | Tracking | R. S. Kothari et al | Ellseg: An ellipse segmentation framework for robust gaze tracking |
| https://gts.ai/dataset-download/open-eds-dataset/ | 29 | Tracking | R. S. Kothari et al | Ellseg: An ellipse segmentation framework for robust gaze tracking |
| https://cs.rit.edu/~cgaplab/RIT-Eyes/ | 29 | Tracking | R. S. Kothari et al | Ellseg: An ellipse segmentation framework for robust gaze tracking |
| https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2Fdatasets-head-mounted&mode=list | 29 | Tracking | R. S. Kothari et al | Ellseg: An ellipse segmentation framework for robust gaze tracking |
| https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2Fdatasets-head-mounted&mode=lis | 29 | Tracking | R. S. Kothari et al | Ellseg: An ellipse segmentation framework for robust gaze tracking |
| https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/labelled-pupils-in-the-wild-lpw/ | 29 | Tracking | R. S. Kothari et al | Ellseg: An ellipse segmentation framework for robust gaze tracking |
| https://mvrl.cis.rit.edu/riteyes/ | 6 | Tracking | A. K. Chaudhary et al | Temporal rit-eyes: From real infrared eye-images to synthetic sequences of gaze behavior |
| https://zhiminghu.net/hu21_fixationnet.html | 8 | Tracking | L. Chen et al | Real-time gaze tracking with head-eye coordination for head-mounted displays |
| https://zhiminghu.net/hu21_fixationnet.html | 20 | Tracking | Z. Hu et al | Fixationnet: Forecasting eye fixations in task-oriented virtual environments |
| https://github.com/GrumpyZhou/visloc-apr/issues/3 | 70 | Tracking | H. Yu et al | Learning bipartite graph matching for robust visual localization |
| https://www.cs.columbia.edu/CAVE/databases/columbia_gaze/ | 37 | 3D Reconstruction | Liang et al. | Reconstructing 3d virtual face with eye gaze from a single image |
| https://facescape.nju.edu.cn/ | 38 | 3D Reconstruction | Ling et al. | Semantically disentangled variational autoencoder for modeling 3d facial details |
| https://whitneypanye.github.io/ | 59 | 3D Reconstruction | Song et al. | A streamable dynamic scene representation with decomposed neural radiance fields |
| https://github.com/facebookresearch/Replica-Dataset | 65 | 3D Reconstruction | Wang et al. | Fully automatic blendshape generation for stylized characters. |
| http://vision.in.tum.de/data/datasets/ | 69 | 3D Reconstruction | Yang et al. | Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation |
| https://www.di.ens.fr/willow/research/surreal/data/ | 66 | 3D Reconstruction | Wang et al. | Parametric model estimation for 3d clothed humans from point clouds |
| https://buff.is.tue.mpg.de/ | 66 | 3D Reconstruction | Wang et al. | Parametric model estimation for 3d clothed humans from point clouds |
| https://cvg.cit.tum.de/data/datasets | 68 | 3D Reconstruction | Xin et al. | Simplemapping: Realtime visual-inertial dense mapping with deep multi-view stereo |
| https://paperswithcode.com/dataset/suncg | 17 | 3D Reconstruction | Gu et al. | Vanishing point aided hash-frequency encoding for neural radiance fields (nerf) from sparse 360Â° input. |
| https://structured3d-dataset.org/ | 53 | 3D Reconstruction | Pintore et al. | Deep scene synthesis of atlanta-world interiors from a single omnidirectional image |
| https://www.cityscapes-dataset.com/benchmarks/ | 51 | 3D Reconstruction | Park et al. | Instant panoramic texture mapping with semantic object matching for large-scale urban scene reproduction |
| https://augmentedperception.github.io/deepviewvideo/ | 59 | Rendering | L. Song et al | Nerfplayer: A streamable dynamic scene representation with decomposed neural radiance fields |
| https://github.com/facebookresearch/Neural_3D_Video | 59 | Rendering | L. Song et al | Nerfplayer: A streamable dynamic scene representation with decomposed neural radiance fields |
| https://github.com/google/hypernerf/releases/tag/v0.1 | 59 | Rendering | L. Song et al | Nerfplayer: A streamable dynamic scene representation with decomposed neural radiance fields |
| https://github.com/dengnianchen/fovnerf?tab=readme-ov-file#dataset | 57 | Rendering | T. Rolff et al | Vrs-nerf: Accelerating neural radiance field rendering with variable rate shading |
| https://github.com/NVlabs/instant-ngp | 57 | Rendering | T. Rolff et al | Vrs-nerf: Accelerating neural radiance field rendering with variable rate shading |
| https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1 | 57 | Rendering | T. Rolff et al | Vrs-nerf: Accelerating neural radiance field rendering with variable rate shading |
| https://cocodataset.org/#home | 63 | Rendering | V. Vasiliu et al | Coherent rendering of virtual smile previews with fast neural style transfer |
| https://dl.acm.org/doi/10.1145/3083187.3083210 | 15 | Image/Video Processing | Chenglei Wu et al | A Dataset for Exploring User Behaviors in VR Spherical Video Streaming |
| https://3dvision.princeton.edu/projects/2012/SUN360/ | 24 | Image/Video Processing | J. Xiao et al | Recognizing scene Recognizing scene viewpoint using panoramic place representation. In CVPR, 2012. |
| http://vcail.kaist.ac.kr/datasets/hahe360/ | 24 | Image/Video Processing | K. Joo et al | Globally optimal inlier set maximization for Atlanta frame estimation. In CVPR, 2018. |
| https://groups.csail.mit.edu/vision/datasets/ADE20K/ | 55 | Image/Video Processing | Bolei Zhou et al | Scene Parsing Through ADE20K Dataset |
| http://www.scan-net.org/ | 55 | Image/Video Processing | Angela Dai et al | ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes |
| https://github.com/niessner/Matterport | 55 | Image/Video Processing | Angel Chang et al | Matterport3D: Learning from RGB-D Data in Indoor Environments |
| https://cs.nyu.edu/~fergus/datasets/nyu_depth_v2.html | 55 | Image/Video Processing | Nathan Silberman et al | Indoor Segmentation and Support Inference from RGBD Images |
| https://github.com/niessner/Matterport | 64 | Image/Video Processing | Angel Chang et al | Matterport3D: Learning from RGB-D Data in Indoor Environments |
| https://github.com/VincentQQu/LFACon | 56 | Image/Video Processing | Qiang Qu | LFACon: Introducing Anglewise Attention to No-Reference Quality Assessment in Light Field Space |
